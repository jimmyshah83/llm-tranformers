[project]
name = "llm-tools"
version = "0.1.0"
description = "Transformers for LLMs"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "huggingface_hub",
    "torch",
    "ipykernel",
    "jupyter",
    "ipywidgets",
    "transformers",
    "datasets",
    "evaluate",
    "accelerate",
    "timm",
    "python-dotenv",
    "tensorboard",
    "trl",
    "peft",
]

[project.optional-dependencies]
# NVIDIA CUDA-focused extras for multi-GPU training
cuda = [
    "deepspeed; platform_system == 'Linux' and platform_machine == 'x86_64'",
    "bitsandbytes; platform_system == 'Linux' and platform_machine == 'x86_64'",
    "flash-attn; platform_system == 'Linux' and platform_machine == 'x86_64'",
]

# Apple Silicon / Metal (MPS) has no extra deps here; use PyTorch MPS backend
metal = []

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]