{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from hf_token.env file\n",
    "load_dotenv('hf_token.env')\n",
    "\n",
    "# Get token from environment variable\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "if token:\n",
    "    login(token=token)\n",
    "    print(\"Successfully logged in to Hugging Face Hub\")\n",
    "else:\n",
    "    print(\"Please set HUGGINGFACE_TOKEN in hf_token.env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "#### Raw Transformers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-4-mini-instruct\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-4-mini-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Clinical and biological significance of RNA N6-methyladenosine regulators in Alzheimer disease?\"\n",
    "outputs = pipeline(prompt, max_new_tokens=100)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "Complete training and evaluation loop for Pytorch models.  \n",
    "You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.\n",
    "\n",
    "This is run on CUDA-enabled NC40ads_H100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/Phi-4-mini-instruct\", num_labels=2, torch_dtype=\"auto\", device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-4-mini-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the Alzheimer PubMed abstracts dataset\n",
    "dataset = load_dataset(\"Gaborandi/Alzheimer_pubmed_abstracts\")\n",
    "\n",
    "# Ensure we have train/validation/test splits\n",
    "if \"validation\" not in dataset:\n",
    "    if \"test\" in dataset and \"train\" in dataset:\n",
    "        # Create a validation split (10% of the current train)\n",
    "        split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "        dataset[\"train\"] = split[\"train\"]\n",
    "        dataset[\"validation\"] = split[\"test\"]\n",
    "    else:\n",
    "        # If only a single split is available, create both validation and test\n",
    "        base_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)  # 20% test\n",
    "        train_temp = base_split[\"train\"]\n",
    "        test_split = base_split[\"test\"]\n",
    "        val_split = train_temp.train_test_split(test_size=0.1111, seed=42)  # ~10% of total for validation\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": val_split[\"train\"],\n",
    "            \"validation\": val_split[\"test\"],\n",
    "            \"test\": test_split,\n",
    "        })\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 values from train dataset\n",
    "print(\"Top 10 values from train dataset:\")\n",
    "top_10 = dataset[\"train\"][:10]\n",
    "for i, (id, title, abstract) in enumerate(zip(top_10[\"pubmed_id\"], top_10[\"title\"], top_10[\"abstract\"])):\n",
    "    print (f\"{i}: id={id} title={title} abstract={abstract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize the text and convert it into PyTorch tensors.\n",
    "\n",
    "# For the Alzheimer's abstracts dataset, combine 'title' and 'abstract'\n",
    "# into a single input text per example.\n",
    "def tokenize_dataset(batch):\n",
    "    titles = batch.get(\"title\", [])\n",
    "    abstracts = batch.get(\"abstract\", [])\n",
    "    texts = [f\"{(t or '').strip()} {(a or '').strip()}\".strip() for t, a in zip(titles, abstracts)]\n",
    "    return tokenizer(texts, truncation=True)\n",
    "\n",
    "# Apply batched mapping to produce input_ids/attention_mask\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 values from train pytorch dataset\n",
    "print(\"Top 10 values from train pytorch dataset:\")\n",
    "top_10 = dataset[\"train\"][:10]\n",
    "for i, (txt, lbl, input_ids, attention_mask) in enumerate(zip(top_10[\"text\"], top_10[\"label\"], top_10[\"input_ids\"], top_10[\"attention_mask\"])):\n",
    "    print (f\"{i}: label={lbl} text={txt} input_ids={input_ids} attention_mask={attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a data collator to create batches of data and pass the tokenizer to it.\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, set up TrainingArguments with the training features and hyperparameters.\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"phi4-rotten-tomatoes\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass all these separate components to Trainer and call train() to start.\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share your model and tokenizer to the Hub with push_to_hub().\n",
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
